# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_training_utils.ipynb.

# %% auto 0
__all__ = ['losses_dict', 'optimizers_dict', 'metrics_dict', 'callback_dict', 'scheduler_dict', 'get_loss_func', 'get_optimizer',
           'padded_cmap', 'compute_metrics', 'show_one_example', 'get_callback_func', 'get_lr_scheduler']

# %% ../nbs/04_training_utils.ipynb 4
from operator import gt, lt
from IPython.display import Audio
from IPython.core.display import display

import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, average_precision_score

import torch
import torchaudio

from .utils import plot_spectrogram, mel_to_wave

# %% ../nbs/04_training_utils.ipynb 5
losses_dict = {
    'ce': torch.nn.CrossEntropyLoss
}

def get_loss_func(loss:str # Key into the losses dictionary
                    ):
    "Getter method to retrieve a loss function"

    assert loss in losses_dict.keys(), f'{loss} is not an existing loss function, choose one from {losses_dict.keys()}.'
    
    return losses_dict[loss]()

# %% ../nbs/04_training_utils.ipynb 7
optimizers_dict = {
    'adamw': torch.optim.AdamW
}

def get_optimizer(optim:str, # Key into the optimizer dictionary
                  model:torch.nn.Module, # The trained model
                  kwargs:dict # Optimizer parameters
                    ):
    "Getter method to retrieve an optimizer"

    assert optim in optimizers_dict.keys(), f'{optim} is not an existing optimizer, choose one from {optimizers_dict.keys()}.'
    
    return optimizers_dict[optim](model.parameters(), **kwargs)

# %% ../nbs/04_training_utils.ipynb 9
def padded_cmap(outputs, labels):
    "Requires one hot encoded labels. Outputs and labels must have the same shape"
    assert outputs.shape == labels.shape, f'Outputs and labels must have the same shape, got {outputs.shape} and {labels.shape} instead.'

    if type(outputs) == torch.Tensor:
        outputs = outputs.detach().cpu().numpy()
    if type(labels) == torch.Tensor:
        labels = labels.detach().cpu().numpy()
    
    pad = np.ones((5, outputs.shape[1]))
    padded_outputs = np.vstack([pad, outputs])
    padded_labels = np.vstack([pad, labels])

    score = average_precision_score(padded_labels, padded_outputs, average='macro')

    return score


# %% ../nbs/04_training_utils.ipynb 10
def compute_metrics(name:str,               # Name of the training stage (train, val, test)
                    outputs:torch.Tensor,   # The output of the model       
                    labels:torch.Tensor,    # The ground truth
                    loss:float,             # The loss of the model
                    example_ct:int,         # Number of examples processed by the model
                    step_ct:int,            # Number of backpropagation steps the model has done
                    epoch:float             # The training epoch
                    )->dict:                # Dictionary of the metrics
        "Compute new metrics from outputs and labels and format existing ones."

        # Transforming logits in probabilities
        outputs = torch.nn.functional.softmax(outputs, dim=1)

        # Transforming labels into one hot encoding
        one_hot_labels = torch.zeros(outputs.size(0), outputs.size(1)).to(labels.device)
        one_hot_labels.scatter_(1, labels.view(-1, 1), 1.)
        labels = one_hot_labels

        # Transforming outputs into one hot encoding
        outputs = torch.zeros_like(outputs).scatter_(1, torch.argmax(outputs, dim=1).unsqueeze(-1), 1.)
        labels, outputs = labels.cpu(), outputs.cpu()

        acc = accuracy_score(labels, outputs)
        prec, recall, f1_weighted, _ = precision_recall_fscore_support(labels, outputs, average='weighted', zero_division=0.0)
        p_cmap = padded_cmap(outputs, labels)

        return {f'{name}/loss': loss,
            f'{name}/example_ct': example_ct,
            f'{name}/step_ct': step_ct,
            f'{name}/epoch': epoch,
            f'{name}/accuracy': acc,
            f'{name}/precision': prec,
            f'{name}/recall': recall,
            f'{name}/f1': f1_weighted,
            f'{name}/padded_cmap': p_cmap
            }

# %% ../nbs/04_training_utils.ipynb 11
metrics_dict = {
    'loss': lt,
    'step': gt,
    'accuracy': gt,
    'precision': gt,
    'recall': gt,
    'f1': gt,
    'padded_cmap': gt
}

# %% ../nbs/04_training_utils.ipynb 19
def show_one_example(data, # The data received by the pytorch dataset
                     outputs:torch.Tensor): # The model prediction
    "A function that shows one input to the model together with its label and prediction"

    inputs, labels, filename = data['input'], data['label'], data['filename']
    print(f'Showing {filename[0]}')
    inputs, labels, outputs = inputs.cpu(), labels.cpu(), outputs.cpu()
    print(f'The shape of the output: {outputs.shape}')
    outputs = torch.nn.functional.softmax(outputs, dim=1)

    print(f'Ground truth: {labels[0]}\nOutputs: {outputs[0]}')
    plot_spectrogram(inputs[0][0], db=True)
    waveform = mel_to_wave(inputs[0][0])
    display(Audio(waveform.numpy(), rate=32000))
    waveform, sample_rate = torchaudio.load(filename[0])
    display(Audio(waveform,  rate=sample_rate))

    

# %% ../nbs/04_training_utils.ipynb 21
callback_dict = {
    '': None,
    'show': show_one_example
}

def get_callback_func(callback:str # Key into the callback dictionary
                    ):
    "Getter method to retrieve a callback function"

    assert callback in callback_dict.keys(), f'{callback} is not an existing callback function, choose one from {callback_dict.keys()}.'
    
    return callback_dict[callback]

# %% ../nbs/04_training_utils.ipynb 23
scheduler_dict = {
    "linear" : (torch.optim.lr_scheduler.LinearLR, {"start_factor" : None, "end_factor" : None, "total_iters" : None, "verbose" : 1}),
    "reduce_lr_on_plateau" : (torch.optim.lr_scheduler.ReduceLROnPlateau, {"patience" : 5, "verbose" : 1}),
    "cosine" : (torch.optim.lr_scheduler.CosineAnnealingLR, {"T_max" : 100, "eta_min" : 1e-9, "verbose" : 1})
}

def get_lr_scheduler(scheduler: str, optimizer,  cnfg: dict):
    "getter method retrieve learning rate scheduler"
    assert scheduler in scheduler_dict.keys(), f"{scheduler} is not an existing scheduler, choose one from {scheduler_dict.keys()}"
    
    for key in cnfg.keys():
        if key in scheduler_dict[scheduler][1].keys():
            scheduler_dict[scheduler][1][key] = cnfg[key]
            
    scheduler_dict[scheduler][1]["optimizer"] = optimizer            
    return scheduler_dict[scheduler][0](**scheduler_dict[scheduler][1])
