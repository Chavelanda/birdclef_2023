{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7583629-8354-4875-b0bb-935f35f22de0",
   "metadata": {},
   "source": [
    "# trainer\n",
    "\n",
    "> A module used to train, validate and test pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d27f3-b905-4aad-9921-a1ce1b9c4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9614b4-afa9-4be5-8021-9dc662c2663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b919a-bb15-40ec-ba38-00e3edc4ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from birdclef.dataset import get_dataloader\n",
    "from birdclef.network import get_model\n",
    "from birdclef.training_utils import get_optimizer, get_loss_func, get_callback_func,get_lr_scheduler, compute_metrics, metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad54d9-7f4f-4c25-9842-61f0f7934710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_weights(model, # A pytorch model\n",
    "                artifact_name, # The name of the artifact\n",
    "                config # wandb config\n",
    "                ):\n",
    "    \"A method to log artifacts into wandb\"\n",
    "\n",
    "    model_artifact = wandb.Artifact(\n",
    "        artifact_name, type=\"model\",\n",
    "        metadata=dict(config))\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{artifact_name}.pth\")\n",
    "    \n",
    "    model_artifact.add_file(f\"{artifact_name}.pth\")\n",
    "\n",
    "    wandb.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406a170-b7a0-4e9b-b1ba-279b729ee165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_one_epoch(model,                  # A pytorch model\n",
    "                    train_dl,               # A pytorch dataloader\n",
    "                    loss_func,              # A function to compute the loss\n",
    "                    optimizer,              # The optimizer\n",
    "                    device,                 # The device where the training is executed ('cpu'|'cuda')\n",
    "                    epoch,                  # The epoch the model is training\n",
    "                    example_ct,             # The number of examples the model has been trained on\n",
    "                    step_ct,                # The number of backpropagation steps the model has done\n",
    "                    n_steps_per_epoch,      # The number of steps for each epoch\n",
    "                    callback_step,          # Steps indicating when the callback function must be called\n",
    "                    callback_func,          # Callback function\n",
    "                    scheduler_step,         # steps indicating when to call the learning rate scheduler\n",
    "                    scheduler_metric,       # metrics tu update the learning rate\n",
    "                    scheduler               # the learning rate scheduler\n",
    "                    ):\n",
    "    \"Train a pytorch model for one epoch\"\n",
    "\n",
    "    model.train()\n",
    "    progress_bar = tqdm(range(len(train_dl)))\n",
    "\n",
    "    for step, data in enumerate(train_dl):\n",
    "        inputs, labels = data['input'], data['label']\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        train_loss = loss_func(outputs, labels)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        example_ct += len(inputs)\n",
    "\n",
    "        epoch_number = (step + 1) / n_steps_per_epoch + epoch\n",
    "        metrics = compute_metrics('train', outputs, labels, train_loss, example_ct, step_ct, epoch_number)\n",
    "        \n",
    "        if (step + 1)%scheduler_step == 0:\n",
    "            if type(scheduler) is torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
    "                scheduler.step(metrics[f\"train/{scheduler_metric}\"])\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        if (step + 1) < n_steps_per_epoch:\n",
    "            # Log train metrics to wandb\n",
    "            wandb.log(metrics)\n",
    "        # Run callback func\n",
    "        if callback_func is not None and step_ct % callback_step == 0:\n",
    "            callback_func(data, outputs)\n",
    "\n",
    "        step_ct += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    return metrics, example_ct, step_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68270c5-7af6-47b4-8ab3-9b8b1f7adf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_model(model, # A pytorch model\n",
    "                   valid_dl, # A pytorch dataloader\n",
    "                   loss_func, # The loss function\n",
    "                   device, # The device where the training is executed ('cpu'|'cuda')\n",
    "                   epoch, # The epoch the model has been trained\n",
    "                   example_ct, # The number of examples the model has been trained on\n",
    "                   step_ct, # The number of backpropagation steps the model has done\n",
    "                   dataset_type='val' # The name of the dataset used\n",
    "                  ):\n",
    "    \"Test or validate a pytorch model\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    metrics = {}\n",
    "    labels_acc = []\n",
    "    outputs_acc = []\n",
    "    loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(range(len(valid_dl)))\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(valid_dl):\n",
    "            inputs, labels = data['input'], data['label']\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss += loss_func(outputs, labels) * labels.size(0)\n",
    "\n",
    "            # Add labels and outputs to acc\n",
    "            labels_acc.append(labels)\n",
    "            outputs_acc.append(outputs)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    # Divide loss by dataset length\n",
    "    val_loss = loss / len(valid_dl.dataset)\n",
    "\n",
    "    labels = torch.cat(labels_acc, dim=0)\n",
    "    outputs = torch.cat(outputs_acc, dim=0)\n",
    "\n",
    "    metrics = compute_metrics(dataset_type, outputs, labels, val_loss, example_ct, step_ct, epoch)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86eb8e1-6df7-4332-be53-8d497b41df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train(conf = None # Wandb configurations containing all hyperparameters\n",
    "          ):\n",
    "    \"Train, validate and test a model using the given configurations\"\n",
    "\n",
    "    with wandb.init(conf) as run:\n",
    "        config = wandb.config\n",
    "        run.name = f\"{config.run_name}\"\n",
    "\n",
    "        # Checking that the defined metric exist\n",
    "        assert config.metric in metrics_dict, f'{config.metric} is not an existing metric, choose one from {metrics_dict.keys()}.'\n",
    "\n",
    "        # Getting dataloaders\n",
    "        train_dl = get_dataloader(config.train_key, config.train_kwargs)\n",
    "        valid_dl = get_dataloader(config.val_key, config.val_kwargs)\n",
    "        test_dl = get_dataloader(config.test_key, config.val_kwargs)\n",
    "\n",
    "        # Getting model, optimizer and loss function\n",
    "        model = get_model(config.model_key, num_classes=train_dl.dataset.num_classes)\n",
    "        model.to(config.device)\n",
    "        optimizer = get_optimizer(config.optimizer_key, model, config.optimizer_kwargs)\n",
    "        loss_func = get_loss_func(config.loss_key)\n",
    "        callback_func = get_callback_func(config.callback_key)\n",
    "        config.lr_scheduler_kwargs[\"total_iters\"] = (len(train_dl)*config.epochs)//config.lr_scheduler_kwargs[\"scheduler_step\"]\n",
    "        config.lr_scheduler_kwargs[\"T_max\"] = (len(train_dl)*config.epochs)//config.lr_scheduler_kwargs[\"scheduler_step\"]\n",
    "        lr_scheduler = get_lr_scheduler(config.lr_scheduler_key, optimizer, config.lr_scheduler_kwargs)\n",
    "\n",
    "        n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.train_kwargs['batch_size'])\n",
    "\n",
    "        # Counters\n",
    "        example_ct = 0\n",
    "        step_ct = 0\n",
    "\n",
    "        best_val = None\n",
    "        for epoch in range(config.epochs):\n",
    "            print(f\"Training epoch {epoch}\")\n",
    "            # Train\n",
    "            metrics, example_ct, step_ct = train_one_epoch(model, train_dl, loss_func, optimizer, config.device, epoch, example_ct, step_ct, n_steps_per_epoch, config.callback_step, callback_func, config.lr_scheduler_kwargs[\"scheduler_step\"], config.lr_scheduler_kwargs[\"scheduler_metric\"], lr_scheduler)\n",
    "\n",
    "            print(\"\\tFinished training. Starting validation\")\n",
    "\n",
    "            # Validate\n",
    "            val_metrics = validate_model(model, valid_dl, loss_func, config.device, epoch + 1, example_ct, step_ct)\n",
    "\n",
    "            print('\\tFinshed validation')\n",
    "\n",
    "            # Log train and validation metrics to wandb\n",
    "            wandb.log({**metrics, **val_metrics})\n",
    "\n",
    "            print(\"\\tMetrics logged to wandb\")\n",
    "\n",
    "            # If the best metric is reached, save the artifact\n",
    "            if best_val is None or metrics_dict[config.metric](val_metrics[f'val/{config.metric}'], best_val):\n",
    "                print(f'\\t{config.metric} in the validation set has improved!')\n",
    "                best_val = val_metrics[f'val/{config.metric}']\n",
    "                best_example, best_step, best_epoch = example_ct, step_ct, epoch\n",
    "                best_model = copy.deepcopy(model)\n",
    "                log_weights(model, config.run_name, config)\n",
    "\n",
    "        print(\"\\tTesting with best model\")\n",
    "        # Test best model\n",
    "        test_metrics = validate_model(best_model, test_dl, loss_func, config.device, best_epoch, best_example, best_step, dataset_type=\"test\")\n",
    "\n",
    "        # Load test metrics as summary\n",
    "        for key in test_metrics.keys():\n",
    "            wandb.summary[key] = test_metrics[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e810cf6",
   "metadata": {},
   "source": [
    "The configuration parameters (config) used in the train function are accessed using wandb.config and are defined by the user when calling the wandb.init function. Here's a list of the config parameters needed in the provided train function:\n",
    "\n",
    "1. run_name: Name for the run.\n",
    "\n",
    "2. train_key: Key to obtain the training dataset.\n",
    "\n",
    "3. train_kwargs: Additional keyword arguments for obtaining the training dataloader.\n",
    "\n",
    "4. val_key: Key to obtain the validation dataset.\n",
    "\n",
    "5. test_key: Key to obtain the test dataset.\n",
    "\n",
    "6. val_kwargs: Additional keyword arguments for obtaining the validation or test dataloader.\n",
    "\n",
    "7. model_key: Key to obtain the model.\n",
    "\n",
    "8. model_kwargs: Additional keyword arguments for obtaining the model.\n",
    "\n",
    "9. optimizer_key: Key to obtain the optimizer.\n",
    "\n",
    "10. optimizer_kwargs: Additional keyword arguments for obtaining the optimizer.\n",
    "\n",
    "11. loss_key: Key to obtain the loss function to be used.\n",
    "\n",
    "12. device: Device for training (e.g., \"cuda\" for GPU, \"cpu\" for CPU).\n",
    "\n",
    "13. epochs: Number of training epochs.\n",
    "\n",
    "14. metric: Metric to use for determining the best model (e.g., accuracy, f1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa28e37-e5b6-4685-8092-3171fea40da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; \n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
